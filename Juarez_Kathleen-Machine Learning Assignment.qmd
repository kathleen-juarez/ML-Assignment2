---
title: "ML"
format:
  html:
    self-contained: true
    toc: true
    toc-depth: 4
   #number-sections: true
    code-line-numbers: true
    code-fold: show  
    code-tools: true
    code-link: true
    embed-resources: true
    df-print: paged
editor: visual
execute: 
  freeze: auto
  warning: false
  error: true 
editor_options: 
  chunk_output_type: inline
---

# 1. Install Package

```{r}
#install.packages("AmesHousing")
#install.packages("MASS")
#install.packages("earth")
#install.packages("caret")
#install.packages("broom")
#install.packages("leaps")
#install.packages("skimr")
#install.packages("tidyverse")
#install.packages("mgcv")
#install.packages("glmnet")
library(glmnet)
library(mgcv)
library(caret)
library(earth)
library(skimr)
library(dplyr)
library(ggplot2)
library(magrittr)
library(broom)
library(tidyverse)
library(MASS)   # for the Boston dataset
library(leaps)
```

# 2. Data

## 2.1 Boston Housing Data

-   `crim`: per capita crime rate by town.

-   `zn`: proportion of residential land zoned for lots over 25,000 sq.ft.

-   `indus`: proportion of non-retail business acres per town.

-   `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

-   `nox`: nitrogen oxides concentration (parts per 10 million).

-   `rm`: average number of rooms per dwelling.

-   `age`: proportion of owner-occupied units built prior to 1940.

-   `dis`: weighted mean of distances to five Boston employment centres.

-   `rad`: index of accessibility to radial highways.

-   `tax`: full-value property-tax rate per \$10,000.

-   `ptratio`: pupil-teacher ratio by town.

-   `black`: 1000(Bk−0.63)21000(Bk−0.63)2 where `Bk` is the proportion of blacks by town.

-   `lstat`: lower status of the population (percent).

`medv`: median value of owner-occupied homes in \$1000s.

```{r}
library(MASS)
data("Boston")

head(Boston)
```

```{r}
skim(Boston)
```

```{r}
# Can visualize few relationships
ggplot(data = Boston, aes(x=lstat, y=medv)) +
  geom_point () + 
  geom_smooth(method="loess")

 #lstat= lower status of the population (percent). medv = median value of owner-occupied homes in $1000s.
```

## 2.2 Chicago Housing Data

![](ChiHousingimage.png)

```{r}
# Chicago Housing Data
library(readr)

chicago <- read_csv("chicago housingdata.csv")

head(chicago)
```

```{r}
ggplot(chicago,aes(x=BEDROOM, y=Ln_Price)) +
  geom_point () + 
  geom_smooth(method="gam")
```

## 2.3 Heart data

Description:

-   Age

-   Sex (1= male, 0= female)

-   ChestPain (1= typical angina, 2= atypical angina, 3= non-anginal pain, 4= asymptotic) RestBP

-   Chol

-   Fbs (fasting blood sugar)

-   RestECG (0 = normal, 1 = having ST-T wave abnormally, 2= left ventricular) CHD (coronary heart disease: 1= yes, 0= no)

```{r}
library(haven)
heart <- 
  read_sav("Heart Data.sav")

View(heart)
```

# 3. Multiple Linear and Logistic Regression

## 3.1 Log-Level Model with Boston Data

```{r}
# Boston Data
library(broom)

lm(log(medv) ~ ., data=Boston) %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), ~ round(., 4)))

```

::: callout-note
## Tip

Log Linear model

-   log(y) = β₀ + β₁x₁ + β₂x₂ + … + ε

    -   A **one-unit increase in `xᵢ`** is associated with an **approximately (100 × βᵢ)% change in `y`**, **holding other variables constant**.
:::

## 3.2 Log-Log Model with Chicago Data

```{r}
# Chicago Housing Data
lm(Ln_Price ~ ., data=chicago) %>% 
  tidy() %>% 
   mutate(across(where(is.numeric), ~ round(., 4)))
```

::: callout-note
## Log Log Model

Log-Log model

-   log(y) = β₀ + β₁log(x₁) + β₂x₂ + … + ε

    -   A **1 % increase in `x1`** is associated with an **approximately β1% change in `y`**, **holding other variables constant**.
:::

## 3.3 Functional Forms Summary Table

| **Model Type** | **Formula** | **Interpretation** |
|------------------------|------------------------|------------------------|
| **Log-Linear** | `log(y) ~ x` | A 1-unit increase in `x` → \~**100 × β%** change in `y` |
| **Log-Log** | `log(y) ~ log(x)` | A 1% increase in `x` → \~**β%** change in `y` (**elasticity**) |
| **Linear-Log** | `y ~ log(x)` | A 1% increase in `x` → \~**β ÷ 100** unit change in `y` |
| **Linear** | `y ~ x` | A 1-unit increase in `x` → **β** unit change in `y` |

### 3.3.1 Caveat for log-level model

log(y) = 0.5·x + …

-   In the Log-level model, it is only accurate when β is small (say \|β\| \< 0.1). For larger coefficients, the **exponential form** gives the exact percentage change.

When the coefficient (like **0.5**) is **large**, the usual shortcut for interpreting the result isn't very accurate.

#### **1. Approximate method** (only works well when β is small — like \< 0.1):

-   Rule of thumb: Multiply the coefficient by 100.\

    → So: `0.5 × 100 = 50%`\

    → **1 unit increase in x = \~50% increase in y**

#### **2. Exact method** (always accurate):

-   Use the formula: (exp(β)−1) x 100\

    → So: (exp(.5) x 100≈64.87\

    → **1 unit increase in x = \~64.9% increase in y**

If your coefficient is **small** (like 0.05), the **approximate method** (β × 100) is fine.

If your coefficient is **large** (like 0.5 or more), use the **exact method** with `exp(β) - 1`.\

```{r}
(exp(0.1) - 1) * 100
(exp(0.5) - 1) * 100
```

## 3.4 Logistic model with Heart Data

```{r}
# Running binary logistic model for heart
m3 <- 
  glm(CHD ~ ., family = binomial(link="logit"), data=heart) #CHD (coronary heart disease: 1= yes, 0= no)
tidy(m3) %>% 
   mutate(across(where(is.numeric), ~ round(., 4)))
```

::: {.callout-note collapse="true" title="code explanation"}
### code explanation

```         
glm(): This fits a generalized linear model.

CHD ~ .: You're modeling CHD (coronary heart disease) as the outcome, using all other variables in the heart dataset as predictors.

family = binomial(link = "logit"): This tells R to run a logistic regression (used when the outcome is binary: 0 or 1).

    Here, CHD = 1 means the person has heart disease.

    CHD = 0 means they don't.

Result: This model estimates how each predictor affects the log-odds of having coronary heart disease.
```
:::

```{r}
# You can calculate OR and their confidence intervals for interpretation
OR_table <- 
 cbind(OR = coef(m3), confint(m3)) %>%
  exp() %>%
  round(3)
```

# 3.5 Boston Data

```{r}
# Boston Housing Data : Best Subsets Regression 

# Load data
data("Boston")

# Fit best subsets regression
best.sub.1 <- 
  regsubsets(log(medv) ~ ., data = Boston, nvmax = 10)

# Show summary
summary(best.sub.1)
```

::: {.callout-note collapse="true" title="code explanation"}
## code explanation

### `regsubsets()`

-   This function is from the **`leaps`** package.

-   It performs **best subsets regression**, which tries **all possible combinations** of predictors to find the **best model** based on a criterion (like adjusted R², BIC, or Cp).

-   It's a tool for **variable selection**.

### `log(medv) ~ .`

-   The **response variable** is `log(medv)` (log of median home value).

-   The `.` means use **all other variables** in the `Boston` dataset as predictors.

`nvmax = 10`

-   This limits the search to models with **up to 10 predictors**

-   Useful for controlling complexity and computation time.
:::

```{r}
reg.summary <-
  summary(best.sub.1)

reg.summary$rsq
```

```{r}
par(mfrow=c(1,3)) 

plot(reg.summary$rss, xlab='Number of Variables', ylab='RSS', type='l') 
# rss = residual sum of squares

plot(reg.summary$adjr2, xlab='Number of Variables', ylab='Adjusted R^2', type='l')

plot(reg.summary$bic, xlab='Number of Variables', ylab='BIC', type='l')
```

::: callout-note
## code explanation

```         
par can be used to set or query graphical parameters. Parameters can be set by specifying them as arguments to par in tag = value form, or by passing them as a list of tagged values
```
:::

### 3.5.2

```{r}
# Chicago Housing Data : Best Subsets Regression 
best.sub. <-
  regsubsets(Ln_Price ~ ., data = chicago, nvmax=10)

summary(best.sub.1)
```

```{r}
reg.summary <- 
  summary(best.sub.1)

reg.summary$rsq
```

```{r}
par(mfrow=c(1,3))

plot(reg.summary$rss, xlab='Number of Variables', ylab='RSS', type='l')
plot(reg.summary$adjr2, xlab='Number of Variables', ylab='Adjusted R^2', type='l')
plot(reg.summary$bic, xlab='Number of Variables', ylab='BIC', type='l')
```

# 4. MARS - Multivariate Adaptive Regression Splines:

## 4.1 MARS with Boston Data

```{r}
# Sample splitting into Train and Test

set.seed(2022)
index <- sample(1:nrow(Boston), size = floor(0.7*nrow(Boston)), replace = FALSE) 
train <- Boston[index,]
test <- Boston[-index,]
dim(train)
```

```{r}
dim(test)
```

```{r}
dim(Boston)
```

```{r}
x <- train[, -14]
y <- train[, 14]
dim(x)
```

```{r}
dim(y)
```

```{r}
y |> as_tibble() |> dim()
```

```{r}
library(caret)
# Create parameter tuning grid
parameter_grid <- floor(expand.grid(degree = 1:2, nprune = seq(2:8)))

# Perform cross-validation using caret package
cv_mars_boston <- train(x=x, y=y,
                        method ='earth',
                        metric='RMSE',
                        trControl = trainControl(method='cv', number = 10),
                        tuneGrid=parameter_grid)

ggplot(cv_mars_boston)
```

```{r}
cv_mars_boston
```

```{r}
mars_predict <- 
  predict(object=cv_mars_boston$finalModel,
                        newdata = test)
mars_predict
```

```{r}
mean(abs(test$medv - mars_predict))
```

```{r}
# Multivariate Adaptive Regression Splines 
mars.1 <- 
  earth(medv ~ ., degree=1, nprune = 5, data=Boston) #nprune: maximum no. of terms including intercept.

summary(mars.1)
```

```{r}
plotmo(mars.1) # out of 13 variables, only 3 were picked.
```

```{r}
mars.2 <-  
  earth(log(medv) ~ ., degree=1, nprune = 7, data=Boston)
summary(mars.2)
```

```{r}
plotmo(mars.2)
```

# 5. GAM; Regression Model

-   **`s()`**

    -   `s(variable)` tells `mgcv::gam()` to fit a **smooth function** (usually a spline) for that predictor.

    -   Instead of forcing a straight-line relationship (like in linear regression), it lets the model **learn a smooth curve** that best fits the data.

```{r}
# Regression Model

# You can quickly check the relationship: whether linear or non-linear

library(mgcv)

gam.1 <-
  gam(log(medv)~ s(crim) + s(rm) + s(dis) + s(ptratio) + s(lstat), data=Boston) # s(): Defining smooths in GAM formulae: polynominal

par(mfrow=c(2,3))
plot(gam.1, se=TRUE, col="blue")

summary(gam.1)
```

::: callout-note
## summary

-   Tests if the smoothingng non-linear effect adds contributions to the model.

```         
Anova for Parametric Effects
            Df  Sum Sq Mean Sq F value    Pr(>F)    
s(crim)      1 25.8737 25.8737 798.011 < 2.2e-16 ***
s(rm)        1 20.9302 20.9302 645.540 < 2.2e-16 ***
s(dis)       1  0.5191  0.5191  16.011 7.283e-05 ***
s(ptratio)   1  1.8794  1.8794  57.967 1.404e-13 ***
s(lstat)     1 11.3096 11.3096 348.817 < 2.2e-16 ***
Residuals  485 15.7250  0.0324                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

-   Tests if the non-linear effect is significant

```         
Anova for Nonparametric Effects
            Npar Df  Npar F     Pr(F)    
(Intercept)                              
s(crim)           3  4.1278  0.006603 ** 
s(rm)             3 27.9782 < 2.2e-16 ***
s(dis)            3  2.7640  0.041479 *  
s(ptratio)        3  2.5208  0.057254 .  
s(lstat)          3  9.4583 4.397e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
:::

## 5.2 GAM; Classification Model (Your turn):

```{r}
library(haven)
heart <- read_sav("Heart Data.sav")

```

```{r}
library(mgcv)

gam2 <- 
  mgcv::gam(I(CHD)~ Age + Sex + ChestPain + RestBP + Chol + RestECG, family = binomial, data=heart)

summary(gam2)

```

```{r}
par(mfrow=c(2,3))
plot(gam2, se=TRUE, col="blue")
```

```{r}
gam3 <-
  gam(I(CHD)~ s(Age) + Sex + ChestPain + s(RestBP) + s(Chol) + RestECG, family = binomial, data=heart)
#summary(gam3)
```

```{r}
par(mfrow=c(2,3))
plot(gam3, se=TRUE, col="blue")
```

```{r}
anova(gam2, gam3, test ="Chisq")

```

```{r}
BIC(gam2)
```

```{r}
BIC(gam3)
```

# 6. Regularization

## 6.1 Lasso & Ridge using glmnet with crime data

```{r}
library(readr)
data <- read_csv("crime.csv")
View(crime_data)
```

```{r}
skim(data)
```

```{r}

#separate x and y
xdata <- 
  model.matrix(ViolentCrimesPerPop ~ ., crime_data)[,-1] # model.matrix creates a design (or model) matrix, e.g., by expanding factors to a set of dummy variables (depending on the contrasts) and expanding interactions similarly. [,-1]: take out y variable

dim(xdata)

head(xdata)

names(xdata)
```

```{r}
ydata <-  data$ViolentCrimesPerPop

# Random split of data
set.seed(340)
itrain <- sample(1:nrow(data), nrow(data)*0.5)
length(itrain)
```

```{r}


itest <- (-itrain)


#separate into training and test data
xtrain = xdata[itrain,]
ytrain = ydata[itrain]

xtest = xdata[itest,] 
ytest = ydata[itest]
```

### 6.1.1 Fitting Lasso, alpha = 1 Crime Data

-   Lasso regularization penalizes the size of coefficients, and the penalty strength is controlled by λ.

-   High λ = stronger shrinkage → simpler model (more coefficients shrunk to 0).

-   `cv.glmnet(...)` runs **Lasso regression** (`alpha = 1`) with 10-fold cross-validation.

-   `type.measure = "mse"`: Uses **Mean Squared Error** as the evaluation metric.

-   `standardize = TRUE` by default: Each predictor is standardized to mean 0 and sd 1 before fitting

```{r}
library(glmnet)
# Step 1: Create a grid of λ (lambda) values
# By default glmnet uses lambda = 10^10 =10,000,000,000  and 10^-2 = 0.01
# present plots in log scale 
grid <- 10^seq(5,-5, length=1000) # grid, 

# Step 2: Run 10-fold cross-validation to find optimal λ
set.seed(123)
cv.lasso <- cv.glmnet(xtrain, ytrain, alpha=1, lambda=grid, 
                     nfolds=10, type.measure = "mse")
# default setting is standardize = True

# Step 3: Plot the cross-validation results
par(mfrow=c(1,1))
plot(cv.lasso)
```

```{r}
cv.lasso


# Step 4: Extract optimal λ values
lambda1 <- cv.lasso$lambda.min # Best model (smallest MSE)
lambda2 <- cv.lasso$lambda.1se # Simpler model (within 1 SE of best)
lambda1

lambda2
```

```{r}
# Step 5: Fit models at both λ values
fit.min <-  glmnet(xtrain, ytrain, alpha=1, lambda = lambda1)
coef(fit.min)

fit.1se <- 
  glmnet(xtrain, ytrain, alpha=1, lambda = lambda2)
coef(fit.1se)
```

```{r}
# Step 6: Predict on test data and calculate MSE
lasso.yhat <-  predict(fit.min, s=lambda1, newx=xtest)
mean((lasso.yhat - ytest)^2)

lasso.yhat <-  predict(fit.1se, s=lambda2, newx=xtest)
mean((lasso.yhat - ytest)^2)
```

```{r}
# Step 7: converting sparse matrix to matrix and to data frame
coef(fit.min) %>% as.matrix() %>% as.data.frame() %>%
  filter(s0 !=0)

coef(fit.1se) %>% as.matrix() %>% as.data.frame() %>%
  filter(s0 !=0)
```

```{r}
# Step 8: Fit Lasso without cross-validation
cv.lasso_min <- glmnet(x=xtrain, y=ytrain, alpha=1)

plot(cv.lasso_min, xvar = "lambda")
abline(v = -log(cv.lasso$lambda.min), col = "red", lty = "dashed")
abline(v = -log(cv.lasso$lambda.1se), col = "red", lty = "dashed")
```

::: callout-note
## note

| Function         | Cross-validation? | Purpose                |
|------------------|-------------------|------------------------|
|                  
 `glmnet(...)`     | ❌ No             | Lasso path             |
| `cv.glmnet(...)` | ✅ Yes            | Finds optimal λ via CV |
:::

### 6.1.2 

```{r}
# find lamda via cross validiation.

grid = 10^seq(5,-5, length=1000) # grid, 
set.seed(123)
cv.ridge = cv.glmnet(xtrain, ytrain, alpha=0, lambda=grid, 
                     nfolds=10, type.measure = "mse")
# alpha = 0  Ridge; default setting is standardize = True

plot(cv.ridge)
```

```{r}
cv.ridge
lambda1 <-  cv.ridge$lambda.min
lambda2 <- cv.ridge$lambda.1se
lambda1
lambda2
fit.min <-  glmnet(xtrain, ytrain, alpha=0, lambda=lambda1)
coef(fit.min)
```

## 6.2 Regularization using AmesHosing Data: Data Preparation (Your Turn)

```{r}
# AmesHousing::make_ames() - processed version of the data
data("ames_raw")
head(ames_raw)
```

```{r}
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)


lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train) # show multi colinearity 
```

```{r}
# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept

ames_train_x <- model.matrix(Sale_Price ~ ., ames_train)[, -1]
ames_train_y <- log(ames_train$Sale_Price)

ames_test_x <- model.matrix(Sale_Price ~ ., ames_test)[, -1]
ames_test_y <- log(ames_test$Sale_Price)

# (ames_train_x)  What is the dimension of of your feature matrix? 
dim(ames_train_x)
```

## 6.3  Regularization - classification, Heart Data, Preparation

```{r}
# Classification model
heart
#separate x and y
xdata <-  model.matrix(CHD ~ ., data)[,-1]
ydata <-  as.factor(data$CHD)

# Random split of data
set.seed(340)
itrain <- sample(1:nrow(data), nrow(data)*0.6)
itest <- (-itrain)


#separate into training and test data
xtrain = xdata[itrain,]
ytrain = ydata[itrain]

xtest = xdata[itest,] 
ytest = ydata[itest]

```

```{r}
grid = 10^seq(5,-5, length=500) # grid, 
set.seed(123)
cv.lasso = cv.glmnet(xtrain, as.factor(ytrain), alpha=1, lambda=grid, 
                     nfolds=10, family = "binomial", type.measure = "class")
# type.measure = "response" gives the probabilities
# type.measure = "class" provide 0 and 1 based on predicted probabilities


plot(cv.lasso)
```

```{r}
# Plot
plot(fit.heart, xvar = "lambda", label = TRUE)
abline(v = -log(cv.lasso$lambda.min), col = "red", lty = "dashed")
abline(v = -log(cv.lasso$lambda.1se), col = "red", lty = "dashed")
```

```{r}
lasso.yhat <-  predict(fit.heart, s=lambda2, newx=xtest, type = "class")

ab <- cbind(lasso.yhat,as.data.frame(ytest))
table(ab$ytest, ab$`s=0.04648734`)

mean(ab$ytest == ab$`s=0.04648734`) #actual == predicted
```
