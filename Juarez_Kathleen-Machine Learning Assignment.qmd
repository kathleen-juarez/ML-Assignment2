---
title: "ML"
format:
  html:
    self-contained: true
    toc: true
    toc-depth: 4
   #number-sections: true
    code-line-numbers: true
    code-fold: show  
    code-tools: true
    code-link: true
    embed-resources: true
    df-print: paged
editor: visual
execute: 
  freeze: auto
  warning: false
editor_options: 
  chunk_output_type: inline
---

# 1. Install Package

```{r}
#install.packages("AmesHousing")
#install.packages("MASS")
#install.packages("earth")
#install.packages("caret")
#install.packages("broom")
#install.packages("leaps")
#install.packages("skimr")
#install.packages("tidyverse")
#install.packages("mgcv")
library(mgcv)
library(caret)
library(earth)
library(skimr)
library(dplyr)
library(ggplot2)
library(magrittr)
library(broom)
library(tidyverse)
library(MASS)   # for the Boston dataset
library(leaps)
```

# 2. Data

## 2.1 Boston Housing Data

-   `crim`: per capita crime rate by town.

-   `zn`: proportion of residential land zoned for lots over 25,000 sq.ft.

-   `indus`: proportion of non-retail business acres per town.

-   `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

-   `nox`: nitrogen oxides concentration (parts per 10 million).

-   `rm`: average number of rooms per dwelling.

-   `age`: proportion of owner-occupied units built prior to 1940.

-   `dis`: weighted mean of distances to five Boston employment centres.

-   `rad`: index of accessibility to radial highways.

-   `tax`: full-value property-tax rate per \$10,000.

-   `ptratio`: pupil-teacher ratio by town.

-   `black`: 1000(Bk−0.63)21000(Bk−0.63)2 where `Bk` is the proportion of blacks by town.

-   `lstat`: lower status of the population (percent).

`medv`: median value of owner-occupied homes in \$1000s.

```{r}
library(MASS)
data("Boston")

head(Boston)
```

```{r}
skim(Boston)
```

```{r}
# Can visualize few relationships
ggplot(data = Boston, aes(x=lstat, y=medv)) +
  geom_point () + 
  geom_smooth(method="loess")

 #lstat= lower status of the population (percent). medv = median value of owner-occupied homes in $1000s.
```

## 2.2 Chicago Housing Data

![](ChiHousingimage.png)

```{r}
# Chicago Housing Data
library(readr)

chicago <- read_csv("chicago housingdata.csv")

head(chicago)
```

```{r}
ggplot(chicago,aes(x=BEDROOM, y=Ln_Price)) +
  geom_point () + 
  geom_smooth(method="gam")
```

## 2.3 Heart data

Description:

-   Age

-   Sex (1= male, 0= female)

-   ChestPain (1= typical angina, 2= atypical angina, 3= non-anginal pain, 4= asymptotic) RestBP

-   Chol

-   Fbs (fasting blood sugar)

-   RestECG (0 = normal, 1 = having ST-T wave abnormally, 2= left ventricular) CHD (coronary heart disease: 1= yes, 0= no)

```{r}
library(haven)
heart <- 
  read_sav("Heart Data.sav")

View(heart)
```

# 3. Multiple Linear and Logistic Regression

## 3.1 Log-Level Model with Boston Data

```{r}
# Boston Data
library(broom)

lm(log(medv) ~ ., data=Boston) %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), ~ round(., 4)))

```

::: callout-note
## Tip

Log Linear model

-   log(y) = β₀ + β₁x₁ + β₂x₂ + … + ε

    -   A **one-unit increase in `xᵢ`** is associated with an **approximately (100 × βᵢ)% change in `y`**, **holding other variables constant**.
:::

## 3.2 Log-Log Model with Chicago Data

```{r}
# Chicago Housing Data
lm(Ln_Price ~ ., data=chicago) %>% 
  tidy() %>% 
   mutate(across(where(is.numeric), ~ round(., 4)))
```

::: callout-note
## Log Log Model

Log-Log model

-   log(y) = β₀ + β₁log(x₁) + β₂x₂ + … + ε

    -   A **1 % increase in `x1`** is associated with an **approximately β1% change in `y`**, **holding other variables constant**.
:::

## 3.3 Functional Forms Summary Table

| **Model Type** | **Formula** | **Interpretation** |
|----|----|----|
| **Log-Linear** | `log(y) ~ x` | A 1-unit increase in `x` → \~**100 × β%** change in `y` |
| **Log-Log** | `log(y) ~ log(x)` | A 1% increase in `x` → \~**β%** change in `y` (**elasticity**) |
| **Linear-Log** | `y ~ log(x)` | A 1% increase in `x` → \~**β ÷ 100** unit change in `y` |
| **Linear** | `y ~ x` | A 1-unit increase in `x` → **β** unit change in `y` |

### 3.3.1 Caveat for log-level model

log(y) = 0.5·x + …

-   In the Log-level model, it is only accurate when β is small (say \|β\| \< 0.1). For larger coefficients, the **exponential form** gives the exact percentage change.

When the coefficient (like **0.5**) is **large**, the usual shortcut for interpreting the result isn't very accurate.

#### **1. Approximate method** (only works well when β is small — like \< 0.1):

-   Rule of thumb: Multiply the coefficient by 100.\

    → So: `0.5 × 100 = 50%`\

    → **1 unit increase in x = \~50% increase in y**

#### **2. Exact method** (always accurate):

-   Use the formula: (exp(β)−1) x 100\

    → So: (exp(.5) x 100≈64.87\

    → **1 unit increase in x = \~64.9% increase in y**

If your coefficient is **small** (like 0.05), the **approximate method** (β × 100) is fine.

If your coefficient is **large** (like 0.5 or more), use the **exact method** with `exp(β) - 1`.\

```{r}
(exp(0.1) - 1) * 100
(exp(0.5) - 1) * 100
```

## 3.4 Logistic model with Heart Data

```{r}
# Running binary logistic model for heart
m3 <- 
  glm(CHD ~ ., family = binomial(link="logit"), data=heart) #CHD (coronary heart disease: 1= yes, 0= no)
tidy(m3) %>% 
   mutate(across(where(is.numeric), ~ round(., 4)))
```

::: {.callout-note collapse="true" title="code explanation"}
### code explanation

```         
glm(): This fits a generalized linear model.

CHD ~ .: You're modeling CHD (coronary heart disease) as the outcome, using all other variables in the heart dataset as predictors.

family = binomial(link = "logit"): This tells R to run a logistic regression (used when the outcome is binary: 0 or 1).

    Here, CHD = 1 means the person has heart disease.

    CHD = 0 means they don't.

Result: This model estimates how each predictor affects the log-odds of having coronary heart disease.
```
:::

```{r}
# You can calculate OR and their confidence intervals for interpretation
OR_table <- 
 cbind(OR = coef(m3), confint(m3)) %>%
  exp() %>%
  round(3)
```

# 3.5 Boston Data

```{r}
# Boston Housing Data : Best Subsets Regression 

# Load data
data("Boston")

# Fit best subsets regression
best.sub.1 <- 
  regsubsets(log(medv) ~ ., data = Boston, nvmax = 10)

# Show summary
summary(best.sub.1)
```

::: {.callout-note collapse="true" title="code explanation"}
## code explanation

### `regsubsets()`

-   This function is from the **`leaps`** package.

-   It performs **best subsets regression**, which tries **all possible combinations** of predictors to find the **best model** based on a criterion (like adjusted R², BIC, or Cp).

-   It's a tool for **variable selection**.

### `log(medv) ~ .`

-   The **response variable** is `log(medv)` (log of median home value).

-   The `.` means use **all other variables** in the `Boston` dataset as predictors.

`nvmax = 10`

-   This limits the search to models with **up to 10 predictors**

-   Useful for controlling complexity and computation time.
:::

```{r}
reg.summary <-
  summary(best.sub.1)

reg.summary$rsq
```

```{r}
par(mfrow=c(1,3)) 

plot(reg.summary$rss, xlab='Number of Variables', ylab='RSS', type='l') 
# rss = residual sum of squares

plot(reg.summary$adjr2, xlab='Number of Variables', ylab='Adjusted R^2', type='l')

plot(reg.summary$bic, xlab='Number of Variables', ylab='BIC', type='l')
```

::: callout-note
## code explanation

```         
par can be used to set or query graphical parameters. Parameters can be set by specifying them as arguments to par in tag = value form, or by passing them as a list of tagged values
```
:::

### 3.5.2

```{r}
# Chicago Housing Data : Best Subsets Regression 
best.sub. <-
  regsubsets(Ln_Price ~ ., data = chicago, nvmax=10)

summary(best.sub.1)
```

```{r}
reg.summary <- 
  summary(best.sub.1)

reg.summary$rsq
```

```{r}
par(mfrow=c(1,3))

plot(reg.summary$rss, xlab='Number of Variables', ylab='RSS', type='l')
plot(reg.summary$adjr2, xlab='Number of Variables', ylab='Adjusted R^2', type='l')
plot(reg.summary$bic, xlab='Number of Variables', ylab='BIC', type='l')
```

# 4. MARS - Multivariate Adaptive Regression Splines:

## 4.1 MARS with Boston Data

```{r}
# Sample splitting into Train and Test

set.seed(2022)
index <- sample(1:nrow(Boston), size = floor(0.7*nrow(Boston)), replace = FALSE) 
train <- Boston[index,]
test <- Boston[-index,]
dim(train)
```

```{r}
dim(test)
```

```{r}
dim(Boston)
```

```{r}
x <- train[, -14]
y <- train[, 14]
dim(x)
```

```{r}
dim(y)
```

```{r}
y |> as_tibble() |> dim()
```

```{r}
library(caret)
# Create parameter tuning grid
parameter_grid <- floor(expand.grid(degree = 1:2, nprune = seq(2:8)))

# Perform cross-validation using caret package
cv_mars_boston <- train(x=x, y=y,
                        method ='earth',
                        metric='RMSE',
                        trControl = trainControl(method='cv', number = 10),
                        tuneGrid=parameter_grid)

ggplot(cv_mars_boston)
```

```{r}
cv_mars_boston
```

```{r}
mars_predict <- 
  predict(object=cv_mars_boston$finalModel,
                        newdata = test)
mars_predict
```

```{r}
mean(abs(test$medv - mars_predict))
```

```{r}
# Multivariate Adaptive Regression Splines 
mars.1 <- 
  earth(medv ~ ., degree=1, nprune = 5, data=Boston) #nprune: maximum no. of terms including intercept.

summary(mars.1)
```

```{r}
plotmo(mars.1) # out of 13 variables, only 3 were picked.
```

```{r}
mars.2 <-  
  earth(log(medv) ~ ., degree=1, nprune = 7, data=Boston)
summary(mars.2)
```

```{r}
plotmo(mars.2)
```

# 5. GAM; Regression Model

-   **`s()`**

    -   `s(variable)` tells `mgcv::gam()` to fit a **smooth function** (usually a spline) for that predictor.

    -   Instead of forcing a straight-line relationship (like in linear regression), it lets the model **learn a smooth curve** that best fits the data.

```{r}
# Regression Model

# You can quickly check the relationship: whether linear or non-linear

library(mgcv)

gam.1 <-
  gam(log(medv)~ s(crim) + s(rm) + s(dis) + s(ptratio) + s(lstat), data=Boston) # s(): Defining smooths in GAM formulae: polynominal

par(mfrow=c(2,3))
plot(gam.1, se=TRUE, col="blue")

summary(gam.1)
```

::: callout-note
## summary

-   Tests if the smoothingng non-linear effect adds contributions to the model.

```         
Anova for Parametric Effects
            Df  Sum Sq Mean Sq F value    Pr(>F)    
s(crim)      1 25.8737 25.8737 798.011 < 2.2e-16 ***
s(rm)        1 20.9302 20.9302 645.540 < 2.2e-16 ***
s(dis)       1  0.5191  0.5191  16.011 7.283e-05 ***
s(ptratio)   1  1.8794  1.8794  57.967 1.404e-13 ***
s(lstat)     1 11.3096 11.3096 348.817 < 2.2e-16 ***
Residuals  485 15.7250  0.0324                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

-   Tests if the non-linear effect is significant

```         
Anova for Nonparametric Effects
            Npar Df  Npar F     Pr(F)    
(Intercept)                              
s(crim)           3  4.1278  0.006603 ** 
s(rm)             3 27.9782 < 2.2e-16 ***
s(dis)            3  2.7640  0.041479 *  
s(ptratio)        3  2.5208  0.057254 .  
s(lstat)          3  9.4583 4.397e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
:::

## 5.2 GAM; Classification Model (Your turn):

```{r}
library(haven)
heart <- read_sav("Heart Data.sav")

```

```{r}
library(mgcv)

gam2 <- 
  mgcv::gam(I(CHD)~ Age + Sex + ChestPain + RestBP + Chol + RestECG, family = binomial, data=heart)

summary(gam2)

```

```{r}
par(mfrow=c(2,3))
plot(gam2, se=TRUE, col="blue")
```

```{r}
gam3 <-
  gam(I(CHD)~ s(Age) + Sex + ChestPain + s(RestBP) + s(Chol) + RestECG, family = binomial, data=heart)
#summary(gam3)
```

```{r}
par(mfrow=c(2,3))
plot(gam3, se=TRUE, col="blue")
```

```{r}
anova(gam2, gam3, test ="Chisq")

```

```{r}
BIC(gam2)
```

```{r}
BIC(gam3)
```
