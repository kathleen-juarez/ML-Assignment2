---
title: "Juarez-Kathleen - ML Assignment"
format:
  html:
    self-contained: true
    toc: true
    toc-depth: 4
   #number-sections: true
    code-line-numbers: true
    code-fold: show  
    code-tools: true
    code-link: true
    embed-resources: true
    df-print: paged
editor: visual
execute: 
  freeze: auto
  warning: false
  error: true 
editor_options: 
  chunk_output_type: inline
---

# 1. Install Package

```{r}
#install.packages("AmesHousing")
#install.packages("MASS")
#install.packages("earth")
#install.packages("caret")
#install.packages("broom")
#install.packages("leaps")
#install.packages("skimr")
#install.packages("tidyverse")
#install.packages("mgcv")
#install.packages("glmnet")
library(glmnet)
library(mgcv)
library(caret)
library(earth)
library(skimr)
library(dplyr)
library(ggplot2)
library(magrittr)
library(broom)
library(tidyverse)
library(MASS)   # for the Boston dataset
library(leaps)
```

# 2. Data

## 2.1 Boston Housing Data

-   `crim`: per capita crime rate by town.

-   `zn`: proportion of residential land zoned for lots over 25,000 sq.ft.

-   `indus`: proportion of non-retail business acres per town.

-   `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

-   `nox`: nitrogen oxides concentration (parts per 10 million).

-   `rm`: average number of rooms per dwelling.

-   `age`: proportion of owner-occupied units built prior to 1940.

-   `dis`: weighted mean of distances to five Boston employment centres.

-   `rad`: index of accessibility to radial highways.

-   `tax`: full-value property-tax rate per \$10,000.

-   `ptratio`: pupil-teacher ratio by town.

-   `black`: 1000(Bk−0.63)21000(Bk−0.63)2 where `Bk` is the proportion of blacks by town.

-   `lstat`: lower status of the population (percent).

`medv`: median value of owner-occupied homes in \$1000s.

```{r}
library(MASS)
data("Boston")

head(Boston)
```

```{r}
skim(Boston)
```

```{r}
# Can visualize few relationships
ggplot(data = Boston, aes(x=lstat, y=medv)) +
  geom_point () + 
  geom_smooth(method="loess")

 #lstat= lower status of the population (percent). medv = median value of owner-occupied homes in $1000s.
```

## 2.2 Chicago Housing Data

![](ChiHousingimage.png)

```{r}
# Chicago Housing Data
library(readr)

chicago <- read_csv("chicago housingdata.csv")

head(chicago)
```

```{r}
ggplot(chicago,aes(x=BEDROOM, y=Ln_Price)) +
  geom_point () + 
  geom_smooth(method="gam")
```

## 2.3 Heart data

Description:

-   Age

-   Sex (1= male, 0= female)

-   ChestPain (1= typical angina, 2= atypical angina, 3= non-anginal pain, 4= asymptotic) RestBP

-   Chol

-   Fbs (fasting blood sugar)

-   RestECG (0 = normal, 1 = having ST-T wave abnormally, 2= left ventricular) CHD (coronary heart disease: 1= yes, 0= no)

```{r}
library(haven)
heart <- 
  read_sav("Heart Data.sav")

View(heart)
```

# Question 1

::: callout-note
## Question 1

According to your professor, there are
three conditions that must be satisfied if one wants to claim variable A causes variable B.  What are those, and elaborate on them.

ANSWER: To claim Variable A causes Variable B, three things must be true: A and B must be related, A must happen before B, and there must be no other factors explaining the relationship. When these conditions are met, we can say A causes B.
:::

# 3. Multiple Linear and Logistic Regression

## 3.1 Log-Level Model with Boston Data

```{r}
# Boston Data
library(broom)

lm(log(medv) ~ ., data=Boston) %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), ~ round(., 4)))

```

::: callout-note
## Tip

Log Linear model

-   log(y) = β₀ + β₁x₁ + β₂x₂ + … + ε

    -   A **one-unit increase in `xᵢ`** is associated with an **approximately (100 × βᵢ)% change in `y`**, **holding other variables constant**.
:::

## 3.2 Log-Log Model with Chicago Data

```{r}
# Chicago Housing Data
lm(Ln_Price ~ ., data=chicago) %>% 
  tidy() %>% 
   mutate(across(where(is.numeric), ~ round(., 4)))
```

::: callout-note
## Log Log Model

Log-Log model

-   log(y) = β₀ + β₁log(x₁) + β₂x₂ + … + ε

    -   A **1 % increase in `x1`** is associated with an **approximately β1% change in `y`**, **holding other variables constant**.
:::

## 3.3 Functional Forms Summary Table

| **Model Type** | **Formula** | **Interpretation** |
|------------------------|------------------------|------------------------|
| **Log-Linear** | `log(y) ~ x` | A 1-unit increase in `x` → \~**100 × β%** change in `y` |
| **Log-Log** | `log(y) ~ log(x)` | A 1% increase in `x` → \~**β%** change in `y` (**elasticity**) |
| **Linear-Log** | `y ~ log(x)` | A 1% increase in `x` → \~**β ÷ 100** unit change in `y` |
| **Linear** | `y ~ x` | A 1-unit increase in `x` → **β** unit change in `y` |

### 3.3.1 Caveat for log-level model

log(y) = 0.5·x + …

-   In the Log-level model, it is only accurate when β is small (say \|β\| \< 0.1). For larger coefficients, the **exponential form** gives the exact percentage change.

When the coefficient (like **0.5**) is **large**, the usual shortcut for interpreting the result isn't very accurate.

#### **1. Approximate method** (only works well when β is small — like \< 0.1):

-   Rule of thumb: Multiply the coefficient by 100.\

    → So: `0.5 × 100 = 50%`\

    → **1 unit increase in x = \~50% increase in y**

#### **2. Exact method** (always accurate):

-   Use the formula: (exp(β)−1) x 100\

    → So: (exp(.5) x 100≈64.87\

    → **1 unit increase in x = \~64.9% increase in y**

If your coefficient is **small** (like 0.05), the **approximate method** (β × 100) is fine.

If your coefficient is **large** (like 0.5 or more), use the **exact method** with `exp(β) - 1`.\

```{r}
(exp(0.1) - 1) * 100
(exp(0.5) - 1) * 100
```

## 3.4 Logistic model with Heart Data

```{r}
# Running binary logistic model for heart
m3 <- 
  glm(CHD ~ ., family = binomial(link="logit"), data=heart) #CHD (coronary heart disease: 1= yes, 0= no)
tidy(m3) %>% 
   mutate(across(where(is.numeric), ~ round(., 4)))
```

::: {.callout-note collapse="true" title="code explanation"}
### code explanation

```         
glm(): This fits a generalized linear model.

CHD ~ .: You're modeling CHD (coronary heart disease) as the outcome, using all other variables in the heart dataset as predictors.

family = binomial(link = "logit"): This tells R to run a logistic regression (used when the outcome is binary: 0 or 1).

    Here, CHD = 1 means the person has heart disease.

    CHD = 0 means they don't.

Result: This model estimates how each predictor affects the log-odds of having coronary heart disease.
```
:::

```{r}
# You can calculate OR and their confidence intervals for interpretation
OR_table <- 
 cbind(OR = coef(m3), confint(m3)) %>%
  exp() %>%
  round(3)
```

# 3.5 Boston Data

```{r}
# Boston Housing Data : Best Subsets Regression 

# Load data
data("Boston")

# Fit best subsets regression
best.sub.1 <- 
  regsubsets(log(medv) ~ ., data = Boston, nvmax = 10)

# Show summary
summary(best.sub.1)
```

::: {.callout-note collapse="true" title="code explanation"}
## code explanation

### `regsubsets()`

-   This function is from the **`leaps`** package.

-   It performs **best subsets regression**, which tries **all possible combinations** of predictors to find the **best model** based on a criterion (like adjusted R², BIC, or Cp).

-   It's a tool for **variable selection**.

### `log(medv) ~ .`

-   The **response variable** is `log(medv)` (log of median home value).

-   The `.` means use **all other variables** in the `Boston` dataset as predictors.

`nvmax = 10`

-   This limits the search to models with **up to 10 predictors**

-   Useful for controlling complexity and computation time.
:::

```{r}
reg.summary <-
  summary(best.sub.1)

reg.summary$rsq
```

```{r}
par(mfrow=c(1,3)) 

plot(reg.summary$rss, xlab='Number of Variables', ylab='RSS', type='l') 
# rss = residual sum of squares

plot(reg.summary$adjr2, xlab='Number of Variables', ylab='Adjusted R^2', type='l')

plot(reg.summary$bic, xlab='Number of Variables', ylab='BIC', type='l')
```

::: callout-note
## code explanation

```         
par can be used to set or query graphical parameters. Parameters can be set by specifying them as arguments to par in tag = value form, or by passing them as a list of tagged values
```
:::

### 3.5.2

```{r}
# Chicago Housing Data : Best Subsets Regression 
best.sub. <-
  regsubsets(Ln_Price ~ ., data = chicago, nvmax=10)

summary(best.sub.1)
```

```{r}
reg.summary <- 
  summary(best.sub.1)

reg.summary$rsq
```

```{r}
par(mfrow=c(1,3))

plot(reg.summary$rss, xlab='Number of Variables', ylab='RSS', type='l')
plot(reg.summary$adjr2, xlab='Number of Variables', ylab='Adjusted R^2', type='l')
plot(reg.summary$bic, xlab='Number of Variables', ylab='BIC', type='l')
```

# Question 2

::: callout-note
## Question 2

What is internal validity? Explain why the existence of causality among variables in a study enhances the internal validity of the study
findings

ANSWER: Internal validity just means a study shows that one thing really causes another, with no outside factors messing things up. When causality is clear, it makes the results more trustworthy because we know the changes are due to the variables we’re studying, not something else.

Which research design is best in general for internal validity? Explain why. 

ANSWER: The best research design for internal validity is an experimental design, especially randomized controlled trials (RCTs). This is because in RCTs, participants are randomly assigned to different groups, which helps prevent bias and control for other factors. This way, researchers can be sure that any changes they see are actually due to the treatment and not something else.
:::

# Question 3

::: callout-note
## Question 3

Compare and contrast (1) experimental design and (2) survey design in terms of (1) what they are, (2) internal validity, and (3) external validity. How can you improve external validity for
experiments?
:::

# 4. MARS - Multivariate Adaptive Regression Splines:

## 4.1 MARS with Boston Data

```{r}
# Sample splitting into Train and Test

set.seed(2022)
index <- sample(1:nrow(Boston), size = floor(0.7*nrow(Boston)), replace = FALSE) 
train <- Boston[index,]
test <- Boston[-index,]
dim(train)
```

```{r}
dim(test)
```

```{r}
dim(Boston)
```

```{r}
x <- train[, -14]
y <- train[, 14]
dim(x)
```

```{r}
dim(y)
```

```{r}
y |> as_tibble() |> dim()
```

```{r}
library(caret)
# Create parameter tuning grid
parameter_grid <- floor(expand.grid(degree = 1:2, nprune = seq(2:8)))

# Perform cross-validation using caret package
cv_mars_boston <- train(x=x, y=y,
                        method ='earth',
                        metric='RMSE',
                        trControl = trainControl(method='cv', number = 10),
                        tuneGrid=parameter_grid)

ggplot(cv_mars_boston)
```

```{r}
cv_mars_boston
```

```{r}
mars_predict <- 
  predict(object=cv_mars_boston$finalModel,
                        newdata = test)
mars_predict
```

```{r}
mean(abs(test$medv - mars_predict))
```

```{r}
# Multivariate Adaptive Regression Splines 
mars.1 <- 
  earth(medv ~ ., degree=1, nprune = 5, data=Boston) #nprune: maximum no. of terms including intercept.

summary(mars.1)
```

```{r}
plotmo(mars.1) # out of 13 variables, only 3 were picked.
```

```{r}
mars.2 <-  
  earth(log(medv) ~ ., degree=1, nprune = 7, data=Boston)
summary(mars.2)
```

```{r}
plotmo(mars.2)
```

# 5. GAM; Regression Model

-   **`s()`**

    -   `s(variable)` tells `mgcv::gam()` to fit a **smooth function** (usually a spline) for that predictor.

    -   Instead of forcing a straight-line relationship (like in linear regression), it lets the model **learn a smooth curve** that best fits the data.

```{r}
# Regression Model

# You can quickly check the relationship: whether linear or non-linear

library(mgcv)

gam.1 <-
  gam(log(medv)~ s(crim) + s(rm) + s(dis) + s(ptratio) + s(lstat), data=Boston) # s(): Defining smooths in GAM formulae: polynominal

par(mfrow=c(2,3))
plot(gam.1, se=TRUE, col="blue")

summary(gam.1)
```

::: callout-note
## summary

-   Tests if the smoothingng non-linear effect adds contributions to the model.

```         
Anova for Parametric Effects
            Df  Sum Sq Mean Sq F value    Pr(>F)    
s(crim)      1 25.8737 25.8737 798.011 < 2.2e-16 ***
s(rm)        1 20.9302 20.9302 645.540 < 2.2e-16 ***
s(dis)       1  0.5191  0.5191  16.011 7.283e-05 ***
s(ptratio)   1  1.8794  1.8794  57.967 1.404e-13 ***
s(lstat)     1 11.3096 11.3096 348.817 < 2.2e-16 ***
Residuals  485 15.7250  0.0324                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

-   Tests if the non-linear effect is significant

```         
Anova for Nonparametric Effects
            Npar Df  Npar F     Pr(F)    
(Intercept)                              
s(crim)           3  4.1278  0.006603 ** 
s(rm)             3 27.9782 < 2.2e-16 ***
s(dis)            3  2.7640  0.041479 *  
s(ptratio)        3  2.5208  0.057254 .  
s(lstat)          3  9.4583 4.397e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
:::

## 5.2 GAM; Classification Model (Your turn):

```{r}
library(haven)
heart <- read_sav("Heart Data.sav")

```

```{r}
library(mgcv)

gam2 <- 
  mgcv::gam(I(CHD)~ Age + Sex + ChestPain + RestBP + Chol + RestECG, family = binomial, data=heart)

summary(gam2)

```

```{r}
par(mfrow=c(2,3))
plot(gam2, se=TRUE, col="blue")
```

```{r}
gam3 <-
  gam(I(CHD)~ s(Age) + Sex + ChestPain + s(RestBP) + s(Chol) + RestECG, family = binomial, data=heart)
#summary(gam3)
```

```{r}
par(mfrow=c(2,3))
plot(gam3, se=TRUE, col="blue")
```

```{r}
anova(gam2, gam3, test ="Chisq")

```

```{r}
BIC(gam2)
```

```{r}
BIC(gam3)
```

# 6. Regularization

## 6.1 Lasso & Ridge using glmnet with crime data

```{r}
library(readr)
data <- read_csv("crime.csv")
View(crime_data)
```

```{r}
skim(data)
```

```{r}

#separate x and y
xdata <- 
  model.matrix(ViolentCrimesPerPop ~ ., crime_data)[,-1] # model.matrix creates a design (or model) matrix, e.g., by expanding factors to a set of dummy variables (depending on the contrasts) and expanding interactions similarly. [,-1]: take out y variable

dim(xdata)

head(xdata)

names(xdata)
```

```{r}
ydata <-  data$ViolentCrimesPerPop

# Random split of data
set.seed(340)
itrain <- sample(1:nrow(data), nrow(data)*0.5)
length(itrain)
```

```{r}


itest <- (-itrain)


#separate into training and test data
xtrain = xdata[itrain,]
ytrain = ydata[itrain]

xtest = xdata[itest,] 
ytest = ydata[itest]
```

### 6.1.1 Fitting Lasso, alpha = 1 Crime Data

-   Lasso regularization penalizes the size of coefficients, and the penalty strength is controlled by λ.

-   High λ = stronger shrinkage → simpler model (more coefficients shrunk to 0).

-   `cv.glmnet(...)` runs **Lasso regression** (`alpha = 1`) with 10-fold cross-validation.

-   `type.measure = "mse"`: Uses **Mean Squared Error** as the evaluation metric.

-   `standardize = TRUE` by default: Each predictor is standardized to mean 0 and sd 1 before fitting

```{r}
library(glmnet)
# Step 1: Create a grid of λ (lambda) values
# By default glmnet uses lambda = 10^10 =10,000,000,000  and 10^-2 = 0.01
# present plots in log scale 
grid <- 10^seq(5,-5, length=1000) # grid, 

# Step 2: Run 10-fold cross-validation to find optimal λ
set.seed(123)
cv.lasso <- cv.glmnet(xtrain, ytrain, alpha=1, lambda=grid, 
                     nfolds=10, type.measure = "mse")
# default setting is standardize = True

# Step 3: Plot the cross-validation results
par(mfrow=c(1,1))
plot(cv.lasso)
```

```{r}
cv.lasso


# Step 4: Extract optimal λ values
lambda1 <- cv.lasso$lambda.min # Best model (smallest MSE)
lambda2 <- cv.lasso$lambda.1se # Simpler model (within 1 SE of best)
lambda1

lambda2
```

```{r}
# Step 5: Fit models at both λ values
fit.min <-  glmnet(xtrain, ytrain, alpha=1, lambda = lambda1)
coef(fit.min)

fit.1se <- 
  glmnet(xtrain, ytrain, alpha=1, lambda = lambda2)
coef(fit.1se)
```

```{r}
# Step 6: Predict on test data and calculate MSE
lasso.yhat <-  predict(fit.min, s=lambda1, newx=xtest)
mean((lasso.yhat - ytest)^2)

lasso.yhat <-  predict(fit.1se, s=lambda2, newx=xtest)
mean((lasso.yhat - ytest)^2)
```

```{r}
# Step 7: converting sparse matrix to matrix and to data frame
coef(fit.min) %>% as.matrix() %>% as.data.frame() %>%
  filter(s0 !=0)

coef(fit.1se) %>% as.matrix() %>% as.data.frame() %>%
  filter(s0 !=0)
```

```{r}
# Step 8: Fit Lasso without cross-validation
cv.lasso_min <- glmnet(x=xtrain, y=ytrain, alpha=1)

plot(cv.lasso_min, xvar = "lambda")
abline(v = -log(cv.lasso$lambda.min), col = "red", lty = "dashed")
abline(v = -log(cv.lasso$lambda.1se), col = "red", lty = "dashed")
```

::: callout-note
## note

| Function         | Cross-validation? | Purpose                |
|------------------|-------------------|------------------------|
|                  
 `glmnet(...)`     | ❌ No             | Lasso path             |
| `cv.glmnet(...)` | ✅ Yes            | Finds optimal λ via CV |
:::

### 6.1.2 

```{r}
# find lamda via cross validiation.

grid = 10^seq(5,-5, length=1000) # grid, 
set.seed(123)
cv.ridge = cv.glmnet(xtrain, ytrain, alpha=0, lambda=grid, 
                     nfolds=10, type.measure = "mse")
# alpha = 0  Ridge; default setting is standardize = True

plot(cv.ridge)
```

```{r}
cv.ridge
lambda1 <-  cv.ridge$lambda.min
lambda2 <- cv.ridge$lambda.1se
lambda1
lambda2
fit.min <-  glmnet(xtrain, ytrain, alpha=0, lambda=lambda1)
coef(fit.min)
```

## 6.2 Regularization using AmesHosing Data: Data Preparation (Your Turn)

```{r}
# AmesHousing::make_ames() - processed version of the data
data("ames_raw")
head(ames_raw)
```

```{r}
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)


lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train) # show multi colinearity 
```

```{r}
# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept

ames_train_x <- model.matrix(Sale_Price ~ ., ames_train)[, -1]
ames_train_y <- log(ames_train$Sale_Price)

ames_test_x <- model.matrix(Sale_Price ~ ., ames_test)[, -1]
ames_test_y <- log(ames_test$Sale_Price)

# (ames_train_x)  What is the dimension of of your feature matrix? 
dim(ames_train_x)
```

## 6.3  Regularization - classification, Heart Data, Preparation

```{r}
# Classification model
heart
#separate x and y
xdata <-  model.matrix(CHD ~ ., data)[,-1]
ydata <-  as.factor(data$CHD)

# Random split of data
set.seed(340)
itrain <- sample(1:nrow(data), nrow(data)*0.6)
itest <- (-itrain)


#separate into training and test data
xtrain = xdata[itrain,]
ytrain = ydata[itrain]

xtest = xdata[itest,] 
ytest = ydata[itest]

```

```{r}
grid = 10^seq(5,-5, length=500) # grid, 
set.seed(123)
cv.lasso = cv.glmnet(xtrain, as.factor(ytrain), alpha=1, lambda=grid, 
                     nfolds=10, family = "binomial", type.measure = "class")
# type.measure = "response" gives the probabilities
# type.measure = "class" provide 0 and 1 based on predicted probabilities


plot(cv.lasso)
```

```{r}
# Plot
plot(fit.heart, xvar = "lambda", label = TRUE)
abline(v = -log(cv.lasso$lambda.min), col = "red", lty = "dashed")
abline(v = -log(cv.lasso$lambda.1se), col = "red", lty = "dashed")
```

```{r}
lasso.yhat <-  predict(fit.heart, s=lambda2, newx=xtest, type = "class")

ab <- cbind(lasso.yhat,as.data.frame(ytest))
table(ab$ytest, ab$`s=0.04648734`)

mean(ab$ytest == ab$`s=0.04648734`) #actual == predicted
```

# Question 4: Regularization

::: callout-note
## Question 4. 1

What is regularization? Why is it better than the Ordinary Least Squares method?

ANSWER:Regularization is a way to keep a regression model from getting too complicated by adding a penalty for large coefficients. It’s better than OLS because OLS can lead to overfitting, where the model is too closely fitted to the training data and doesn’t work well on new data. Regularization helps make the model simpler and more reliable, focusing on the most important predictors.
:::

::: callout-note
## Question 4.2

Compare and contrast Ridge and Lasso regressions. What are the
similarities and differences? Under what circumstances does each
regularization work better?

ANSWER: Ridge and Lasso are both ways to prevent overfitting by adding penalties to the model, but they do it differently. **Ridge** shrinks all coefficients, keeping all variables in the model but making them less important. **Lasso**, on the other hand, can shrink some coefficients to zero, effectively removing less important variables. **Ridge** is better when you have many relevant predictors and don’t want to eliminate any, while **Lasso** is great when you think only a few predictors matter and want to simplify the model.
:::

::: callout-note
## Question 4.3

What is cross-validation? Explain the logic behind it.

ANSWER: Cross-validation is a method to test how well a model will perform by splitting the data into parts. In k-fold cross-validation, the data is divided into k sections. The model is trained on k-1 of those sections and tested on the last one, and this repeats for each section. The idea is to get a better idea of how the model will perform on new data, helping to avoid overfitting and giving a more reliable performance measure.
:::

::: callout-note
## Question 4.4

How is it related to the regularization, such as Ridge and Lasso regressions? 

ANSWER: Cross-validation helps choose the best regularization strength (like lambda) for Ridge and Lasso by testing different values on different data splits. It makes sure the model isn’t overfitting or underfitting, finding the right balance for better performance on new data.
:::

# Task 1

Run the linear model for the **Chicago Housing Data,** where ***Ln_Price*** is regressed on all other variables in the data. 

ANSWER: The results show that distressed sales (DSALE) are associated with substantially lower home prices, with distressed properties selling for about 33% less than comparable non-distressed homes, holding all other factors constant. In contrast, home size, measured by the natural log of square footage (Ln_SQFT), has a positive and economically meaningful effect: a 1% increase in square footage is linked to roughly a 0.47% increase in price, all else equal.

```{r}
lm_chicago <- lm(Ln_Price ~ ., data = chicago)

# View a summary
summary(lm_chicago)
```

# Task 3

Take a note while you read the **Islam et al. (2022) JBR** article. Organize it well inside the same QMD file. Provide a full citation of the article in the QMF file such that when you render it, the full citation automatically appears at the bottom of the report.

Islam, T., Meade, N., Carson, R. T., Louviere, J. J., & Wang, J. (2022). The usefulness of socio-demographic variables in predicting purchase decisions: Evidence from machine learning procedures. *Journal of Business Research*, 151, 324–338. <https://doi.org/10.1016/j.jbusres.2022.07.004>

## **Purpose of the Study**

-   Reassess the predictive power of socio-demographic variables in purchase decisions.

-   Compare traditional logistic regression to machine learning procedures (MLPs).

-   Investigate:

    1.  Predictive power improvements via MLPs.

    2.  Nonlinear patterns & interactions in socio-demographic effects.

    3.  Whether these relationships have causal significance (propensity score approach).

## **Background & Rationale**

-   Conventional wisdom: demographics have marginal predictive value, especially when past purchase data are available.

-   Arguments for revisiting:

    1.  For new products/new market entrants, past purchase data may be unavailable.

    2.  Demographic data is now cheaper, richer, and more widely available (e.g., online behavioral proxies).

    3.  Past studies relied on linear models, ignoring nonlinearities and interactions.

-   Modern ML can detect complex patterns without rigid assumptions and can avoid overfitting.

## **Context**

-   Socio-demographics have been used to predict food purchases, fast-food choices, organic food purchases, e-commerce, hedonic consumption, concert attendance, renewable energy adoption, etc.

-   Past findings are mixed — predictive power varies by product category/context.

-   Many earlier studies overlooked the low acquisition cost and wide availability of demographic data.

-   Most low-performance findings came from traditional regression-based models that miss nonlinearities and interactions.

## **Machine Learning Procedures (MLPs) Tested**

**Baseline:** Binary logistic regression.

**MLPs:**

1.  **Random Forests** – Ensemble of decision trees, good at capturing nonlinearities and interactions.

2.  **Gradient Boosting (Extreme Gradient Boosting)** – Sequentially improves weak learners.

3.  **Support Vector Machine (SVM)** – Optimal separating hyperplanes, can use nonlinear kernels.

4.  **K-Nearest Neighbor (k-NN)** – Classifies based on proximity to neighbors.

5.  **Neural Network (Single Hidden Layer)** – Flexible nonlinear mapping.

6.  **Genetic Algorithm (GA)** – Evolutionary search optimizing classification accuracy.

7.  **Particle Swarm Optimization (PSO)** – Swarm-based search/classification.

## **Data**

-   **Source:** IRI panel data.

-   **Sample:** \>7,000 panelists.

-   **Products:** Baby formula, single-serving coffee, snacks, detergent powder, toothpaste, canned tuna.

-   Buyers vs. non-buyers sampled.

-   Variables: Standard socio-demographics (household size, race, ethnicity, income, education, presence of children, age, affluence, region).

## **Method**

-   **Prediction Task:** Buy vs. No-buy classification.

-   **Data Split:** 70% training, 30% testing.

-   **Evaluation Metric:** Sensitivity (true positive rate) — preferred over accuracy due to class imbalance.

-   **Hyperparameter tuning:** Cross-validation & grid search.

-   **Causal Inference:** Propensity score weighting to adjust for confounding (boosted tree estimation).

## **Key Findings**

### 1. Predictive Performance

-   All MLPs outperformed logistic regression.

-   Out-of-sample sensitivity improvement: **+20% to +33%** vs. logistic regression.

-   **Top performers:** Random Forests, SVM, Gradient Boosting.

-   **k-NN** lowest among MLPs but still \> logistic regression.

-   Neural network showed overfitting due to relatively small sample size.

### 2. Nonlinear Patterns

-   MLPs uncovered nonlinearities (e.g., inverted U-shaped income effects).

-   Example: Baby formula purchase probability rose with income, then dropped sharply at highest income level — not detected by logistic regression.

-   Interactions between variables (e.g., presence of child × age × income) were important.

### 3. Causality

-   Propensity score analysis reduced confounding and enabled more credible causal inference.

-   Many nonlinear relationships identified by MLPs remained significant after causal adjustment.
